\documentclass{article}

\input{packages}
\input{commands}

\title{Trantor: Modular State Machine Replication}
\author{Matej Pavlovic}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}

We present Trantor, a modular state machine replication (SMR) system.
It decomposes the concept of SMR into multiple smaller components and draws carefully designed, simple yet powerful abstractions around them.
Trantor aims at being practical, not neglecting important technical aspects
such as checkpointing, state transfer, garbage collection, reconfiguration, or weighted voting, making them an integral part of the design.
Trantor's modularity allows it to be flexible, maintainable, adaptable, and future-proof, which are our main design goals.
Components such as the total-order broadcast protocol can easily be swapped in and out, potentially even at runtime.

Even though the focus of Trantor is not on performance, a preliminary performance evaluation of our Byzantine fault-tolerant implementation
shows an attractive  throughput of over 30k tx/s with a latency of under 1.3 seconds (and 0.5 seconds at 5'000 tx/s)
at a moderate scale of 32 replicas dispersed over 5 different continents,
despite a naive implementation of many of Trantor's components.

\end{abstract}

\section{Introduction}

State machine replication (SMR) is a way of achieving fault-tolerant execution of an application
by replicating it across multiple connected machines – nodes of a computer network that we call replicas.
Each replica executes an identical copy of the application.
SMR, and in particular Byzantine fault-tolerant SMR,
which tolerates arbitrary (Byzantine) faults of replicas and clients,
has become a fundamental building block of blockchain systems.

We model the application as a deterministic state machine.
Starting from some initial state, its state is deterministically updated by applying transactions to it.
Thus, the state of the application is completely determined by the initial state and an ordered sequence of transactions known as the transaction log.
It is the SMR system’s task to maintain an identical copy of the ever-growing transaction log on each non-faulty replica
and apply it to the application, despite the faults of some replicas.
An SMR system runs across multiple replicas in a network, each initialized with an application and its initial state.
It  receives transactions, submitted by clients, on its input, orders them,
and ensures that each replica's copy of the application executes all the transactions in the given order.

Many blockchain systems share this SMR architecture, such as Ethereum \cite{ethereum}, Filecoin \cite{filecoin}, The Internet Computer \cite{dfinity}, Aptos \cite{aptos}, and many others.
These systems are usually designed around a single consensus (transaction ordering) protocol that is built in the node software and tightly coupled with other components.
The monolithic design often integrates many concepts from transaction dissemination, through ordering, all the way to execution.
Swapping the execution engine for another one or completely changing the consensus protocol,
with minimal changes to the rest of the system, would probably imply significant design and implementation overhead.

This paper presents Trantor, a novel SMR system.
The main design goal of Trantor is high modularity, not just in its implementation, but at a conceptual level.
The reason we focus on modularity is to enable tailoring an SMR system to different deployment scales,
allowing for different sub-protocol implementations and catering for different application and performance needs.
These goals are especially important in multi-instance SMR / blockchain deployment architectures aiming at scaling bockchain systems, such as Interplanetary Consensus (IPC) \cite{ipc}.

Trantor builds on the idea of separating the dissemination of transaction payloads from ordering
(also used in other systems \cite{modularblockchain,narwhal,bullshark}),
and takes this concept even further.
It consists of multiple loosely coupled components with well-defined interfaces.
Different stages of Trantor’s operation are separate components that interact through asynchronous invocations, as is common when defining distributed abstractions.
E.g., when a new transaction block is appended to the transaction log by the ordering component,
it invokes the fetching component, which reacts by fetching the corresponding transaction data
and invokes the application component to perform the execution.
In addition to making it easy to understand and independently reason about individual components,
such an execution model is particularly well suited for being expressed in the Mir framework \cite{mir} which we use to implement Trantor.

% Marko: Here there is a gap which needs to be massaged, I do not think batching is that relevant to be highlighted here.

Trantor delivers a continuous stream of transaction blocks to the application on each replica,
but its usage of memory and storage does not grow indefinitely.
It is up to the application to process this (infinite) stream of transaction blocks in a way that does not exhaust its resources.
To this end, Trantor periodically asks the application to create a snapshot of its state after the application of a specific prefix of the transaction log.
The application is expected to be able to completely restore its state from such a snapshot.
Trantor includes these snapshots (along with other metadata) in checkpoints.
After creating such a checkpoint, Trantor deletes all data pertaining to the checkpointed prefix of the transaction log.

\begin{wrapfigure}{r}{0.5\textwidth}
    \includegraphics[width=\linewidth]{figures/application-interface.jpeg}
    \caption{Interaction between Trantor and the application it replicates.}
    \label{fig:application-interface}
\end{wrapfigure}

The set of replicas Trantor runs on can dynamically change.
Those changes are driven by the application itself, by passing the replica configuration information
(consisting of identities and network addresses of the replicas) to Trantor at run time.
The initial replica set is Trantor’s configuration parameter.
Trantor then periodically requests the next configuration from the application and parametrizes its components accordingly.

The basic interaction between Trantor and the application is similar in spirit to CometBFT’s ABCI++ interface \cite{abciplusplus}
and is depicted in \cref{fig:application-interface}.
We describe it in detail in \cref{sec:execution}.

In the rest of this document,
we briefly discuss related work in \cref{sec:related-work} and define the system model in \cref{sec:system-model}.
\Cref{sec:overview} presents an overview of Trantor’s high-level architecture and mode of operation.
In \cref{sec:algorithm-details}, we describe Trantor’s components in more detail and discuss some other noteworthy aspects of its design.
We show a preliminary performance evaluation in \cref{sec:evaluation}.
Finally, we conclude in \cref{sec:conclusions}.

\section{Related Work}
\label{sec:related-work}

Trantor proposes a decomposition of an SMR system at the conceptual level,
and follows this decomposition in its system design.
Some systems, notably Sui \cite{sui} with Bullshark \cite{bullshark} as their consensus engine,
already successfully make the point of separating transaction payload dissemination from ordering.
Hyperledger Fabric \cite{fabric} also separates execution from ordering in its execute-order-validate architecture.
Still, swapping different ordering engines is notoriously difficult and until today,
Fabric does not seem to have a proper Byzantine fault-tolerant ordering service.
Trantor takes these concepts further by treating modularity as the first priority.

CometBFT \cite{comet}, powering the Cosmos ecosystem, is an SMR system similar to Trantor in its interaction with the application.
Trantor's application interface follows a similar principle as CometBFT’s ABCI++,
more precisely, the subset of it that drives the application’s state updates.
However, ABCI++ also provides access to the application’s state, making CometBFT a proxy for querying the application state.
Moreover, CometBFT is tightly coupled with the Tendermint consensus protocol that is an integral part of the system.

\section{System Model}
\label{sec:system-model}

This section describes the basic model and assumptions Trantor uses.
In a nutshell, we consider an eventually (partially) synchronous message passing system
consisting of replicas interconnected with reliable authenticated point-to-point links.
We assume that a Byzantine adversary corrupts some of the replicas (we call those faulty)
and controls the scheduling of all network messages (within the constraints of eventual synchrony),
but cannot subvert standard cryptographic primitives, e.g., invert secure hash functions.

We generalize the usual constraint on the extent to which the adversary can corrupt replicas.
Instead of limiting their number (using the classic “f out of n” approach),
we consider each replica to be associated with a weight as part of that replica’s identity.
Roughly, the weight corresponds to the replica’s “voting power” in the system.
The total weight is the sum of the weights of all replicas.
An example of such voting power that Trantor can be parameterized with is replica stake,
as used in Proof-of-Stake protocols. 

We define a weak quorum as any set of replicas whose combined weights exceed one third of the total weight.
Analogously, a strong quorum must exceed two thirds of the total weight.%
\footnote{We could further generalize by defining the quorum thresholds arbitrarily,
as long as a weak and a strong quorum always intersect, but we stick with one third and two thirds for simplicity.}
We assume that the adversary cannot corrupt a weak quorum of replicas.
Note that, in the special case when each replica has the same weight,
our model is equivalent to the adversary corrupting less than one third of the replicas.

\section{Overview}
\label{sec:overview}

\subsection{High-Level Architecture}

We now present a simplified view of Trantor’s high-level architecture.
We intentionally leave out some details and corner cases in order to convey a high-level idea of Trantor’s general operation.
Conceptually, Trantor can be subdivided in 5 stages:

\begin{enumerate}

    \item \textbf{Mempool:} Receive transactions and group them into blocks.

    \item \textbf{Dissemination:} Make transaction blocks available using consistent broadcast \cite{distributedprogrammingbook} augmented by an availability certificate.
    In a nutshell, consistent broadcast ensures that, each time a replica broadcasts a block, each correct replica either receives that block or no block.
    The availability certificate roughly corresponds to making a weak quorum of replicas persistently store the blocks, such that any replica can fetch them in the future.

    \item \textbf{Ordering:} Produce a totally ordered transaction log consisting of the availability certificates created at the dissemination stage.

    \item \textbf{Fetching:} Obtain the transactions using references as they appear in the transaction log.
    The transactions are content-addressed, as those references are derived from hashing the contents of the transactions (and included in the availability certificates).

    \item \textbf{Execution:} Apply the transactions to the application state.

\end{enumerate}

\Cref{fig:basic-operation} shows a simplified diagram of Trantor’s basic operation.
Each stage is the responsibility of a separate component that we describe below.

% Logically, components communicate with other components by emitting events that other components react to.
% Note that we only use the notion of events to model the interaction between Trantor's components
% as is the standard in distributed system literature \cite{distributedprogrammingbook}.
% We do not prescribe a particular communication model for an implementation,
% even though Mir, the general framework for implementing distributed systems \cite{mir} that we use to implement Trantor,
% does directly implement such an event-based model.
% Alternatively (and equivalently), one can see sending an event to a component as asynchronously invoking a function of that component's interface.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/basic-operation}
    \caption{Trantor's basic operation. We will further refine this diagram in \cref{fig:implementation-diagram}, \cref{sec:trantor-modules}.}
    \label{fig:basic-operation}
\end{figure}

\subsubsection{Mempool}
\label{sec:mempool}

The mempool component is responsible for transaction input.
It stores received transactions that are pending for ordering and execution.
Similarly to the application logic, we consider the mempool an external dependency
that only needs to implement an interface for interacting with the rest of the system.
Thus, how transactions are added to the mempool is out of scope of this document and,
from Trantor’s point of view, the transactions simply appear there.

It is important to note that each replica has its own local mempool
that is not expected to be consistent with other replica’s mempools.
There is no guarantee that a transaction appearing in one replica’s mempool will ever appear in another replica’s mempool.
If the same transaction does appear in multiple replicas’ mempools,
all these replicas will add their copy of the transaction to the blocks they disseminate and order.
Trantor removes such duplicates at the fetching stage (see also \cref{sec:fetching}).
Methods for removing these duplicates early on and saving resources can be implemented in a future version of Trantor.

The basic operation of the mempool is very simple.
Upon \invoke{RequestNewBlock},
it selects all the transactions that need to be proposed and responds by invoking \invoke{NewBlock} (with those transactions) at the \module{Disseminator}.
The transactions within a block are ordered with respect to each other.
When a block of transactions reaches the execution stage, the transactions will be executed in this order.

\subsubsection{Dissemination}

The replica’s dissemination component has two tasks:
\begin{enumerate}
    \item Transmitting locally stored transaction blocks to other replicas to ensure their global availability and
    \item Retrieving blocks that are available, but not stored locally by the replica
\end{enumerate}


For ensuring transaction availability, this component obtains transactions from the mempool and broadcasts them to other replicas.
For each individual broadcast block,
the broadcast primitive implemented by the dissemination component is that of consistent broadcast,
which ensures that no two correct replicas will observe a different block
(i.e., prevents even faulty replicas from equivocating).
Moreover, the implementation also produces an availability certificate for each disseminated block.
An availability certificate is a proof of retrievability of a transaction block that can be verified by any other replica.
A correct replica must be able to obtain the block only using information found in the certificate.

In a nutshell, whenever the dissemination component is ready to disseminate a transaction block,
it invokes \invoke{RequestNewBlock} at the mempool.
Upon receiving a block through the corresponding \invoke{NewBlock} invocation,
the dissemination component executes a protocol that sends the block to other replicas,
asking them to persistently store it and return a signed acknowledgement.
The resulting (multisig) availability certificate then consists of a reference to (hash of) the block
and a list of those signed acknowledgements from a weak quorum.
The dissemination component then invokes \invoke{NewCert} with the certificate
when requested through \invoke{RequestCert} by the ordering component.

The dissemination component’s second task is retrieving blocks based on a given availability certificate.
This functionality is triggered by a \invoke{RequestBlock} invocation containing a certificate.
Here, the component simply downloads the missing transactions from any of the replicas
that signed the certificate and invokes \invoke{ProvideBlock} with those transactions at the fetching component.
Since each availability certificate is always signed by at least one correct replica, the retrieval is always guaranteed to succeed.

\subsubsection{Ordering}

The ordering component takes availability certificates produced by the dissemination component
and orders them into a sequence (called the transaction log) that is guaranteed to be the same on all replicas.
This is done using multiple parallel instances of the PBFT protocol \cite{pbft},
multiplexed together akin to the ISS protocol \cite{iss}, albeit without eliminating duplicate transactions during ordering. 

The ordering component waits for \invoke{NewCert} invocations
(containing availability certificates) coming from the dissemination component,
broadcasts (i.e., proposes) them using a total-order broadcast protocol (PBFT in our case),
and invokes \invoke{OrderedCert} at the fetching component, each invocation representing one entry in the transaction log.

\subsubsection{Fetching}
\label{sec:fetching}

The fetching component transforms a sequence of availability certificates received through \invoke{OrderedCert} invocations
into a corresponding sequence of transaction blocks and passes them on to the execution component invoking \invoke{OrderedBlock}.

In general, a significant number of the availability certificates delivered by a replica have been proposed by other replicas,
and thus the referenced transaction blocks might not be present locally.
The fetching component’s task is to obtain all those blocks from other replicas.
It does so by exchanging pairs of \invoke{RequestBlock} / \invoke{ProvideBlock} invocations with the dissemination component.

The fetching component also filters out transactions that have already been included in earlier blocks,
such that every transaction is only passed to the execution component once (see \cref{sec:tx-deduplication}).

\subsubsection{Execution}

The execution component contains the state and logic of the application provided by the user.
It mainly receives ordered blocks through \invoke{OrderedBlock} invocations and applies the contained transactions to the application.

The execution component is also involved in checkpointing and configuring the set of replicas, which we discuss in \cref{sec:algorithm-details}.

\subsection{Epoch-Based Operation and Configuration}
\label{sec:epochs-and-configuration}

We express the progress of the whole system through the length of the transaction log.
Each ordered block has a position in the transaction log that we call height.
Note that the whole replicated state of the system can be identified by a single number
– the height of the last applied transaction block.
The first block has height 0.

\subsubsection{Checkpoints}

To capture the replicated state at a certain height, Trantor uses checkpoints.
A checkpoint is a verifiable snapshot of the replicated state.
It summarizes the application of all the ordered blocks up to a certain height.
We say a checkpoint is at height $h$ if it comprises the first $h$ blocks of the log.
A checkpoint completely replaces a prefix of the transaction log,
i.e., a replica that obtains a checkpoint for height $h$ (summarizing blocks $0$ to $h-1$)
can re-initialize its state and directly continue ordering and executing the block at height $h$.

A checkpoint contains a certificate that any replica can verify
to confirm that the checkpointed state was indeed agreed upon.
The certificate consists of a cryptographic hash of the checkpointed state
signed by a strong quorum of replicas that agreed on that state.

\subsubsection{Epochs}

An epoch is a contiguous section of the transaction log.%
\footnote{An epoch in Trantor is not to be confused with a Filecoin epoch,
which corresponds to the notion of height in Trantor.}
Sometimes, very informally, we abuse the term epoch to also refer to the period of time
during which this section is produced (note that Trantor does not explicitly deal with the notion of time%
\footnote{Exposing a basic notion of agreed-upon real wall clock time to the application
is a feature that can be added to a future version of Trantor.}
).
An epoch has a well-defined length in terms of ordered blocks.
For example, if the first epoch’s length is 4, it comprises the heights 0, 1, 2, and 3,
and the second epoch starts with height 4.
The corresponding transaction log is depicted in \cref{fig:transaction-log}.

A replica actively participates in the distributed protocols pertaining to only one epoch at a time.
Only once it fills the whole epoch’s transaction log does it advance to the next epoch.
While in principle it is possible to run multiple epochs concurrently, we stick to one epoch at a time for simplicity.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/transaction-log}
    \caption{The very beginning of the transaction log.
    Depicted is the first epoch of length 4, a checkpoint and the beginning of the second epoch.
    Each entry in the transaction log consists of an availability certificate (signed by a quorum of replicas)
    that refers to a transaction block using a hash of the block.}
    \label{fig:transaction-log}
\end{figure}

\subsubsection{Configuration}
\label{sec:configuration}

The configuration of the whole system (including the set of replicas) is fixed throughout an epoch,
but can change from one epoch to another.
At the end of an epoch, i.e., after having applied the last transaction block to its state,
Trantor requests a new configuration from the application (execution component) to use for future epochs.
This configuration is part of the replicated state and thus is also included in checkpoints.
Trantor creates a checkpoint at the start of each epoch,
summarizing the replicated state as it was at the end of the previous epoch.
New replicas joining the system then only need to obtain the starting checkpoint
of the first epoch the configuration of which they are part of.

To better streamline Trantor’s operation,
a configuration provided by the application at the end of epoch $e$ is not used directly in epoch $e+1$,
but rather in epoch $e+1+\param{configOffset}$, where \param{configOffset} is a system parameter.
A checkpoint thus contains the configuration for not just the epoch it starts,
but also for \param{configOffset} additional subsequent epochs.
Correspondingly, Trantor uses the initial configuration for the first $\param{configOffset}+1$ epochs.

\subsection{Access Control and Input Validation}
\label{sec:access-control-input-validation}

Trantor does not authenticate the sources of received transactions and implements no access control.
We believe this is not the task of the core of an SMR system and should be implemented on top of it.

However, it is sometimes useful for the implementation of the consensus protocol to be able to validate a received proposal early
(e.g., as soon as it is received from another replica), so that the consensus protocol can be aborted early for invalid proposals.
Apart from opening the door to potentially important performance optimizations,
such a mechanism might also be required for preventing certain forms of denial-of-service attacks.

Trantor provides the necessary tools for the application to support such early proposal validation.
The ordering component supports the integration of an optional external \module{ProposalValidator} module,
allowing an early abortion of the ordering of transactions that this module (as injected dependency) considers invalid.

\section{Algorithm Details}
\label{sec:algorithm-details}

This section describes the algorithms we use to implement Trantor’s components in more detail
and explains some additional noteworthy aspects of Trantor’s design.

\subsection{Dissemination}
\label{sec:dissemination}

In each epoch, a subset of Trantor’s replicas are designated as leaders by the ordering component
(see MirBFT \cite{mirbft} and ISS \cite{iss} for details).
Each leader replica’s dissemination component continuously executes the following simple algorithm.

\begin{enumerate}
\item Invoke \invoke{RequestNewBlock} at the mempool.
\item Wait for obtaining a transaction block through \invoke{NewBlock} invoked by the mempool.
\item Send a \msg{RequestSig} message to all other replicas (including itself), containing the obtained block.
\item Upon reception of a \msg{RequestSig} message,
persistently store the transaction block and sign a digest of all its contents
(our implementation uses ECDSA over the Secp256k1 curve).
\item Respond with a \msg{Sig} message containing the computed signature.
\item Upon reception of valid signatures from a weak quorum (including itself),
construct a multisig availability certificate out of those signatures and the block digest,
and store the certificate in a buffer.
(A weak quorum of signatures ensures that there is at least one correct replica the data can be fetched from later,
making the block available.)
\item If the buffer is not yet full, restart from 1.
\end{enumerate}

When \invoke{RequestCert} is invoked by the ordering component,
the dissemination component responds by invoking \invoke{NewCert}
containing all the availability certificates present in its buffer, empties the buffer,
and restarts the above dissemination algorithm in case it has stopped due to a full buffer.

With this approach, many multisig certificates are included in a single \invoke{NewCert} invocation.
This allows for a continuous dissemination of transaction blocks
even before the ordering component requests any certificates.
Once \invoke{RequestCert} is invoked, the ordering component can respond immediately with the buffered certificates
and the ordering can start without further delay.

The size of the buffer is a configuration parameter
and also determines the maximal number of multisig certificates included in one \invoke{NewCert} invocation.
For simplicity, we refer to such a block of certificates as a single availability certificate
downstream from the dissemination component.
Any operation applied to it is, under the hood, applied to all the sub-certificates.
We consider an availability certificate valid iff all the contained sub-certificates are valid.

\subsection{Ordering}

Trantor’s ordering component establishes a total order on availability certificates
received from the dissemination component using a protocol that multiplexes multiple instances of PBFT.
It is inspired by the ISS protocol \cite{iss}, but forgoes transaction deduplication for simplicity
(it can be added in a future version of Trantor).

In short, at the start of each epoch, Trantor assigns each height of that epoch to one leader replica
and initializes one instance of PBFT per leader.
The leader is configured to be the first PBFT primary charged with proposing availability certificates
for all the heights (“sequence numbers” in PBFT terminology) assigned to it.

The liveness of each individual PBFT instance guarantees
that each height will eventually be assigned an availability certificate.
If the leader fails (or there is asynchrony in the network),
the remaining correct replicas agree on a special empty certificate (that does not refer to any transactions) instead. Once all heights of an epoch have been assigned availability certificates,
Trantor advances to the next epoch, repeating the whole process.
For details on the ordering protocol, we refer the interested reader to ISS \cite{iss}.

\subsection{Creating Checkpoints}
\label{sec:creating-checkpoints}

The creation of checkpoints is performed by a dedicated sub-protocol, but is orchestrated by the ordering component.
Immediately after a transition from epoch $e-1$ to $e$, before any blocks are appended to the transaction log,
the ordering component creates a new instance of the checkpointing protocol,
parametrized with a snapshot of the system state comprised of:
\begin{compactitem}
\item The current state of the application as obtained from the execution component
\item Relevant state of the Trantor protocol itself,
such as epoch number, height, and other variables that need to be reinitialized when restoring the state
\item Configuration of epoch $e$ and the \param{configOffset} following epochs
\item Information about processed transactions (needed for deduplication, see \cref{sec:tx-deduplication})
\end{compactitem}
Each replica of epoch $e-1$ then executes a simple checkpointing protocol akin to the one used in PBFT:
\begin{enumerate}

\item Compute a digest of the snapshot and sign it.

\item Send the signed digest to all other replicas.

\item Wait for the reception of matching signed digests from a strong quorum%
\footnote{Using a weak quorum would be sufficient under our assumptions,
since it would still guarantee a correct replica storing a provably correct (thanks to the certificate) checkpoint.
However, since we use checkpoints as a condition for a replica to advance epochs,
requiring a strong quorum limits the number of slow replicas that can fall behind to less than a weak quorum.}
(including self).

\item Construct a (multisig) checkpoint certificate from the snapshot digest and the collected signatures.

\item The checkpoint itself consists of the state snapshot and the checkpoint certificate.

\end{enumerate}

Once the checkpoint is ready, the ordering component saves it in its state, replacing any older checkpoints.
Moreover, the ordering component also informs the application (execution component) about the new checkpoint.
This can be useful to the application for various reasons, e.g., creating backups consistently with other replicas.

Note, however, that the constructed checkpoint certificates, all being valid,
may contain different signatures on different replicas.
In other words, each replica may create a different valid certificate,
depending on which signatures the replica received first.
In order to adhere to the paradigm of providing identical inputs to all copies of the application on all replicas,
Trantor creates another instance of a consensus protocol (also PBFT in our case)%
\footnote{In our modular implementation of Trantor, we use (different instances of) the very same PBFT module
to agree on both availability certificates and checkpoints - an example of the benefits of modularity.}
to agree on one single version of the certificate.
This certificate is then used by all replicas for the checkpoint delivered to the application.

\subsection{Garbage Collection and State Transfer}
\label{sec:garbate-collection-state-transfer}

A checkpoint for epoch $e$ (i.e, appearing between epochs $e-1$ and $e$ in the transaction log)
contains all information necessary for a replica to start / continue operating from the beginning of epoch $e$.
Thus, as soon as such a checkpoint is available to a replica, it garbage-collects all data related to earlier epochs.
The rationale is that the effect of any message that would need to be sent for any of the previous epochs is subsumed by the checkpoint which,
if needed, can be transmitted instead.

Any replica that falls behind (e.g., due to transient network failures or simply due to being slow) can completely recover its state from a checkpoint.
To this end, each replica keeps track of the highest epoch it observed in communication received from each other replica.
When a replica $r$ notices the epoch of another replica $r'$ being lower by a predefined threshold (Trantor’s system parameter),
$r$ sends its latest checkpoint to $r'$.
$r'$ does not need to wait for any quorum to confirm the validity of the checkpoint, as it can verify the received checkpoint’s certificate.

Note that this naive state transfer algorithm is correct, but rather inefficient,
as it leads to multiple correct replicas sending the same checkpoint (a potentially bulky data structure) to the same straggling replica.
This inefficiency can easily be optimized away by only notifying the straggler about the own progress
and letting the straggler request the checkpoint data from only one replica at a time (or even request chunks of it from multiple replicas).
Another possibility is storing the checkpoint in an external data store available to all replicas
and only transfer a reference (e.g., an IPLD pointer) to it between replicas.

Since checkpoint (multisig) certificates consist of a list of signatures, the verification relies on the knowledge of the signers’ public keys.
If the set of replicas (i.e., membership) never changes, all replicas can easily verify checkpoint certificates
based on their knowledge of all other replica’s public keys.
If, however, we want to allow the system to dynamically reconfigure the membership (see \cref{sec:execution})
and, at the same time, not place any assumptions on the correctness of replicas removed from the replica set,
verifying checkpoint certificates becomes more difficult due to possible long-range / ``i still work here'' attacks \cite{longrangeattacks}.
Handling this case exceeds the scope of this document and will be added to a future version of Trantor.%
\footnote{If Trantor is used in an IPC \cite{ipc} subnet, for example, the parent subnet within the IPC hierarchy can naturally serve as a trusted source of membership information.}

\subsection{Transaction Identification and Post-Order Deduplication}
\label{sec:tx-deduplication}

In order to apply each transaction exactly once, Trantor needs to keep track of which transactions have been applied in the past.
To this end, each transaction is uniquely identified by a tuple (\textit{ClientID}, \textit{TxNo})
with ClientID uniquely identifying the entity that created the transaction (the client), e.g., a user of the system.
The transaction number \textit{TxNo} is a number that each client assigns to each transaction it produces.
The client is responsible for producing transactions with monotonically increasing \textit{TxNo}s.
If two transactions with different payloads but the same \textit{TxNo}s are produced by the same client,
Trantor does not guarantee processing any of them.
One (not both) of them, however, still might be applied.

Trantor uses these transaction identifiers at the fetching stage to filter out transactions that have already been executed previously.
For each client, Trantor keeps an entry for all \textit{TxNo}s passed to the execution component.
When the fetching component obtains a block of transactions from the dissemination component, it filters out those for which an entry already exists.

The data structures required by such a simple filter, however, would grow indefinitely and eventually exhaust all the system’s resources.
We therefore introduce the concept of a client watermark window - a range of \textit{TxNo}s a client is allowed to concurrently submit to the system.
If a transaction outside of the client’s watermark window appears in the mempool, Trantor is allowed to ignore it.
At each epoch transition, Trantor sets the start of each known client’s watermark window to the lowest \textit{TxNo} not yet executed.
The window then extends over the next \param{WindowSize} transaction numbers, where \param{WindowSize} is a system parameter.

Leveraging the client watermark window, Trantor only needs a constant amount of memory to represent
all transactions ever executed that originate from a single client.
The total memory overhead is thus $O(c)$, $c$ being the number of clients.
Reducing this overhead even further is possible under some mild additional synchrony assumptions by garbage-collecting information about inactive clients,
but is out of the scope of this document and may be implemented in a future version of Trantor.

\subsection{Execution}
\label{sec:execution}

The execution component is responsible for managing the application state and execution. Its tasks are the following.

\begin{itemize}
    \item Apply transactions contained in ordered blocks to the application logic.
    \item Consume periodically generated checkpoints.
    \item Create snapshots of the application state.
    \item Restore the application state from snapshots.
    \item Dynamically configure the Trantor system (e.g., define the replica membership).
\end{itemize}

The application logic is an external dependency wrapped by Trantor’s execution component.
For convenience, Trantor supports two variants of the application logic: a simplified static one and a full-fledged one with reconfiguration support.

\subsubsection{Simple Static Application}

The simple static variant only needs to implement the following functions:

\begin{itemize}
    \item \code{ApplyTXs(transactions)}: Applies a block of transactions to the state machine.
    \item \code{Snapshot() \textrightarrow{} serialized\_state}: Returns a snapshot of the application state.
    \item \code{RestoreState(checkpoint)}: Restores the application state from a checkpoint.
    \item \code{Checkpoint(checkpoint)}: Announces a new checkpoint to the application.
\end{itemize}

The Snapshot and RestoreState functions are tightly coupled; what Snapshot returns, RestoreState must be able to parse.
The \code{serialized\_state} value (represented as an opaque byte array) returned by Snapshot
will be contained (as one of the fields) in the checkpoint that Trantor passes to RestoreState
(other fields contain additional metadata such as the checkpoint certificate).
The application thus has full control over serializing and deserializing its own state.
This means, in particular, that a large application state need not necessarily be all encoded in the value returned by Snapshot,
but can be, for example, a reference to an external data store (e.g., IPFS) that stores the actual state and to which all replicas have access.

The Checkpoint function is technically not necessary for implementing a textbook version of state machine replication
and can safely be ignored by an application implementation that does not make use of checkpoints.
In practice, however, this feature can be useful, e.g., for backup purposes.

\subsubsection{Reconfigurable Application}

Applications that wish to be able to dynamically reconfigure the Trantor system can use the full-fledged application interface.
On top of the simple one, the full-fledged application interface contains one additional function:
\begin{itemize}
    \item \code{NewEpoch(number) \textrightarrow{} configuration}: Announces the start of a new trantor epoch with number number.
    Returns a configuration to be used by Trantor after \param{configOffset} epochs.
\end{itemize}

We expect the decisions on new configurations to be made by the replicated application logic, based only on the replicated state.
Since Trantor consistently invokes the \code{NewEpoc}h function after having applied the same sequence of transactions on each replica,
we ensure that the configuration decisions will be consistent across different copies of the application at all correct replicas as well.

\subsubsection{Example Execution}

To illustrate the operation of the execution component,
\cref{lst:example-execution} shows the sequence of application function invocations as they would appear at each replica in a system with epoch length 4,
corresponding to the transaction log depicted in \cref{fig:transaction-log}.
\begin{lstlisting}[
  mathescape,
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=lines,
  xleftmargin=0.2\textwidth,
  xrightmargin=0.2\textwidth,
  aboveskip=1em,
  belowskip=1em,
  framexleftmargin=0.1\textwidth,
  framexrightmargin=0.1\textwidth,
  framexbottommargin=1em,
  framextopmargin=1em,
  label=lst:example-execution,
  captionpos=b,
  caption= Application of the application corresponding to \cref{fig:transaction-log}.
]
 1. NewEpoch(0)
 2. ApplyTXs(TX$_0$1, TX$_0$2, ...)
 3. ApplyTXs(TX$_1$1, TX$_1$2, ...)
 4. ApplyTXs(TX$_2$1, TX$_2$2, ...)
 5. ApplyTXs(TX$_3$1, TX$_3$2, ...)
 6. Snapshot()
 7. Checkpoint(checkpoint)
 8. NewEpoch(1)
 9. ApplyTXs(TX$_4$1, TX$_4$2, ...)
10. ApplyTXs(TX$_5$1, TX$_5$2, ...)
11. ...
12. ...
\end{lstlisting}

The purpose of Trantor can be summarized as ensuring that exactly his sequence of application invocations happens consistently at all replicas,
regardless of transactions appearing in the mempools in different orders and some replicas failing or even misbehaving.

Note that Trantor does not invoke the \code{RestoreState} function under normal circumstances.
It is only invoked on replicas that fall behind and need to catch up, or on replicas that freshly joined the system after reconfiguration.
The application must be ready to receive such an invocation at any point in time.
Trantor guarantees, however, that the restoration happens at epoch boundary,
i.e., Trantor only restores a snapshot taken at the end of an epoch, before any transactions from the next epoch have been applied.
The standard sequence of invocations from the start of an epoch then follows (unless state needs to be restored again), as in \cref{lst:restoring-state}.

\begin{lstlisting}[
  mathescape,
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=lines,
  xleftmargin=0.2\textwidth,
  xrightmargin=0.2\textwidth,
  aboveskip=1em,
  belowskip=1em,
  framexleftmargin=0.1\textwidth,
  framexrightmargin=0.1\textwidth,
  framexbottommargin=1em,
  framextopmargin=1em,
  label=lst:restoring-state,
  captionpos=b,
  caption=Restoring application state from checkpoints.
]
 1. NewEpoch(0)
 2. ApplyTXs(TX$_0$1, TX$_0$2, ...)
 3. ApplyTXs(TX$_1$1, TX$_1$2, ...)
 4. ApplyTXs(TX$_2$1, TX$_2$2, ...)
 5. RestoreState(checkpoint)
 6. RestoreState(checkpoint)
 7. NewEpoch(8)
 8. RestoreState(checkpoint)
 9. NewEpoch(532)
10. ApplyTXs(TX$_{...}$1, TX$_{...}$2, ...)
11. ApplyTXs(TX$_{...}$1, TX$_{...}$2, ...)
12. ...
13. ...
\end{lstlisting}

\section{Implementation}
\label{sec:implementation}

We now present our implementation of Trantor, diving deeper into Trantor's architecture and refining the descriptions from \cref{sec:overview,sec:algorithm-details}.
We implement Trantor in Go, using the Mir distributed system implementation framework \cite{mir}.
Note that we are describing the implementation of a \emph{single replica}.
Each replica runs its own copy of what we present in this section.
In the following, we first explain some overarching concepts relevant for all modules and then describe each module in detail.

\subsection{Epochs}

As mentioned in \cref{sec:epochs-and-configuration}, Trantor's operation is epoch-based.
Therefore, several components are, explicitly or implicitly, associated with a concrete epoch.
The \module{Orchestrator} module drives progress from one epoch into the next one.
It instantiates and initializes all the modules necessary for an epoch, assembles the corresponding part of the transaction log from their output,
triggers the creation of checkpoints, and, when the checkpoints are available, triggers garbage collection.

Several modules in \cref{fig:implementation-diagram} (\module{Dissemination}, \module{Ordering}, \module{Checkpointing}) contain submodules.
While the top-level modules are long-lived and exist throughout Trantor's operation,
the submodules are generally only relevant for one epoch, for which the \module{Orchestrator} dynamically instantiates them, and are eventually garbage-collected.

\subsection{Garbage Collection Using the Retention Index}

In Trantor, garbage collection is very explicit.
Each data structure (including whole submodules) that is subject to garbage collection is explicitly associated with an integer \emph{retention index}.
The retention index is a system-wide, monotonically increasing variable driven by the \module{Orchestrator}.
All data associated with a retention index lower than the current one is defined to be obsolete and safe to delete.
In Trantor, the retention index is derived from the epoch number,
resulting in the garbage collection semantics described in \cref{sec:garbate-collection-state-transfer}.

We could have performed garbage collection directly based on the epoch number,
but using the retention index helps keep the semantically different concepts of ``progress in SMR'' and ``garbage collection'' cleanly separated.
This way, modules that otherwise do not have a reason to be aware of the progress of the whole state machine (or even of the concept of SMR)
-- which are, in fact, most of the modules --
can be cleaner, simpler, more general, and thus more reusable.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/implementation-diagram}
    \caption{Schematic view of Trantor's implementation's components.
    A line between two boxes means that the two components interact.
    The four bottom-most components (\module{Networkig}, \module{Timer}, \module{Hashing}, and \module{Crypto}) have a supporting role.
    They interact with many other components and we omit the corresponding lines in the diagram for simplicity of presentation.
    One can see this diagram as a refinement of \cref{fig:basic-operation}.
    }
    \label{fig:implementation-diagram}
\end{figure}

\subsection{Trantor's Modules}
\label{sec:trantor-modules}

\Cref{fig:implementation-diagram} displays all relevant modules of Trantor's implementation and their interactions.
It rather accurately represents the structure of our code, omitting only few technical details not relevant for understanding the workings of the implementation
(e.g., the mechanism for verification of certificates or computing unique identifiers of transactions and transaction blocks).
To improve the presentation, the names of some functions and modules also do not necessarily fully correspond to those used in our code.%
\footnote{We will be consolidating our implementation of Trantor to closely match the more intuitive terminology used in this document,
rather than the other way round (using less intuitive terminology in this document just to match our implementation).}

For simplicity, this description also does not distinguish between modules handling local invocations (received from other modules)
and network messages (from other replicas).
In practice, they are handled slightly differently,
since, unlike local invocations, network messages must always be assumed to be potentially created by a malicious adversary.

\subsubsection{\module{Orchestrator}}

\includegraphics[width=\textwidth]{figures/modules/module-orchestrator.pdf}

The \module{Orchestrator} is the heart (and brain) of Trantor.
Its main task is to advance the transaction log.
In addition to the transaction log data structure, the \module{Orchestrator} maintains the configurations
(including replica membership information) of the current and \param{ConfigOffset} next epochs,
as well as the most recent checkpoint.
It also keeps track of the progress of other replicas in order to decide when they need to be sent a checkpoint to catch up.

Trantor always starts operating from an initial checkpoint (equivalent to a genesis block in some other blockchain systems),
with which the \module{Orchestrator} is initialized.
A checkpoint, by definition, defines the system state at the start of an epoch.
The \module{Orchestrator} thus initializes all its internal data structures
and invokes other modules to start a new epoch.
This includes instructing the \module{Dissemination} and \module{Ordering} modules to create new submodules
executing, respectively, the dissemination and ordering protocols for the new epoch.
The \module{Orchestrator} then waits to be invoked by the \module{Ordering} module announcing the delivery of availability certificates
for all the heights of the epoch, which it relays (in the correct order) to the \module{Block Fetcher}.
Each delivered availability certificate is explicitly annotated with its position in the transaction log
and the \module{Orchestrator} does not expect the certificates to be delivered in order.

At any point in time, the \module{Orchestrator}'s \invoke{RestoreFromCheckpoint(checkpoint])} function may be invoked,
in which case it performs a re-initialization as described above, using the received checkpoint.

The \module{Orchestrator} has the following interface:
\begin{itemize}

    \item \invoke{DeliverCert(height, cert)}\\
    One of the \module{Consensus} modules delivered an availability certificate for some particular height.
    The \module{Orchestrator} inserts the delivered availability certificate
    at the corresponding height in the transaction log.
    The \module{Orchestrator} then announces as many availability certificates from the transaction log as possible
    to the \module{Block Fetcher}, while respecting in-order delivery
    (i.e., announcing the availability certificates according to their heights).

    \item \invoke{NewCheckpoint(checkpoint)}\\
    The \module{Checkpoint} module produced a checkpoint.
    Note that this checkpoint has not yet been agreed upon and a different one may have been produced at each replica (see \cref{sec:creating-checkpoints}).
    The \module{Orchestrator} thus initiates agreement on this new checkpoint.
    To this end, it creates a new instance of a \module{Consensus} module -- more precisely,
    makes the \module{Ordering} module do so -- by invoking \invoke{NewModule} at \module{Ordering},
    with the new checkpoint set as the proposal.

    \item \invoke{DeliverCheckpoint(checkpoint)}\footnote{
        In practice, \invoke{DeliverCert} and \module{DeliverCheckpoint} are the same function invoked by the \module{Consensus} module.
        They only differ in the payload they carry.
        We present them as two different functions for more clarity.
    }\\
    The \module{Consensus} module agreed on a checkpoint.
    The \module{Orchestrator} updates its most recent checkpoint
    and announces the agreed-upon checkpoint to the \module{Block Fetcher}.

    Moreover, the \module{Orchestrator} performs garbage collection of both its internal state
    and several other modules (such as \module{Dissemination}, \module{Ordering}, and others)
    by invoking their corresponding \invoke{GarbageCollect} functions with a retention index
    that corresponds to the epoch of the delivered checkpoint.

    \item \invoke{EpochProgress(replica, epoch)}\\
    Notifies the \module{Orchestrator} about a replica having advanced to a particular epoch.
    The \module{Orchestrator} updates its local view about that replicas progress
    (the maximal epoch it has been seen advancing to).
    The \module{Orchestrator} uses this information when deciding which other replicas need to be sent a checkpoint
    to restore their state from.
    This function is invoked by the \module{Checkpoint} module whenever it receives a message from a replica.
    While other modules could also invoke this function on the reception of an epoch-annotated message,
    we chose the \module{Checkpoint} module, as it only receives one message per epoch per replica.

    \item \invoke{RestoreFromCheckpoint(checkpoint)}\\
    When another replica considers this replica to have fallen behind (by the mechanism explained just above),
    the \module{Orchestrator} will receive the \invoke{RestoreFromCheckpoint} invocation
    (triggered by a message from the other replica).
    If the received checkpoint is indeed more recent than the locally stored latest checkpoint,
    the \module{Orchestrator} initiates validation of the received checkpoint
    by invoking \invoke{ValidateCheckpoint} at the \module{Checkpoint Validator}.

    \item \invoke{CheckpointValidated(result)}\\
    When the \module{Checkpoint Validator} successfully validated a received checkpoint,
    the \module{Orchestrator} re-initializes the whole system from it, as described above.

    \item \invoke{PushCheckpointTimeout}\\
    The \module{Timer} module periodically invokes this function.
    Each time, it checks the observed progress of other replicas and,
    if an of them is locally perceived as having fallen behind,
    sends sends them the latest checkpoint to restore their state from.

    \item \invoke{NewConfig(configData)}\\
    As described in \cref{sec:configuration}, the \module{Application} module defines the system configurations.
    Those have the form of a \invoke{NewConfig} invocations of the \module{Orchestrator},
    which stores each configuration until the epoch in which it needs to be applied.

\end{itemize}

\subsubsection{\module{Proposal Validator}}
\label{sec:module-proposal-validator}

\includegraphics[width=\textwidth]{figures/modules/module-proposal-validator.pdf}

If Trantor is configured to use external input validation (see \cref{sec:access-control-input-validation}),
the \module{Consensus} module may request the \module{Proposal Validator} to validate consensus proposals received from other replicas.
The \module{Proposal Validator}'s implementation can perform arbitrary (potentially application-specific) checks on the proposal
and notifies the \module{Consensus} about their result.

The \module{Proposal Validator} module's interface consists of a single function:

\begin{itemize}
    
    \item \invoke{ValidateProposal(proposal)}\\
    The \module{Proposal Validator} checks the validity of the given proposal and invokes \invoke{ProposalValidated}
    back at the module that invoked \invoke{ValidateProposal} (the \module{Consensus} module in our case).
    Note that nothing prevents the \module{Proposal Validator} from communicating with other modules
    (in particular, with the \module{Application}), or even with other replicas before responding to the validation request.
    
\end{itemize}

\subsubsection{\module{Ordering}}
\label{sec:module-ordering}

\includegraphics[width=\textwidth]{figures/modules/module-ordering.pdf}

The \module{Ordering} module is responsible for managing \module{Consensus} submodules.
In each epoch, it contains one or more submodules that agree on the assignment of availability certificates
to different heights in the transaction log.
Moreover, in each epoch, \module{Ordering} contains one more submodule to agree on the epoch checkpoint.
The \module{Ordering} module creates and destroys the submodules as commanded by the \module{Orchestrator}.

The \module{Ordering} module has the following interface:

\begin{itemize}

    \item \invoke{NewModule(moduleConfig, retentionIndex)}\\
    When invoked by the \module{Orchestrator}, the \module{Ordering} module instantiates a new \module{Consensus} submodule
    (that implements the PBFT protocol in our case).
    The created submodule is associated with the given retention index and parametrized according to \invoke{moduleConfig}.
    Parameters include the membership, the heights for which the submodule needs to reach agreement,
    an optional proposal for each height, and some protocol-specific parameters.%
    \footnote{Even the protocol itself could be chosen by a parameter, even though our implementation only supports PBFT so far.}

    \item \invoke{GarbageCollect(retentionIndex)}\\
    When invoked by the \module{Orchestrator},
    the \module{Ordering} module deletes all its submodules that have been created with a lower retention index than the given one.

\end{itemize}

\subsubsection{\module{Consensus}}

\includegraphics[width=\textwidth]{figures/modules/module-consensus.pdf}

The \module{Consensus} modules are dynamically created and deleted by the \module{Ordering} module
and are responsible for executing the consensus protocol.
A \module{Consensus} module is oblivious of the epoch it is part of or the other \module{Consensus} instances.
It simply receives a list of heights as part of its configuration,
and for each of these heights, it needs to agree on a value and announce it to the \module{Orchestrator} by invoking its \invoke{Deliver} function.

If the \module{Consensus} module is configured with a proposal, it will use that proposal as input for the consensus protocol.
Otherwise, it will request an availability certificate from a \module{Disseminator} module it is configured to use
and proposes that certificate.

The \module{Consensus} module has the following interface:
\begin{itemize}

    \item \invoke{NewCert(cert)}\\
    In case the \module{Consensus} module has not been pre-configured with a proposal
    and requested a new certificate from the \module{Disseminator},
    the \invoke{NewCert} invocation carries a new availability certificate
    that the \module{Consensus} module uses as a consensus proposal.

    \item \invoke{ProposalValidated(result)}\\
    This function is relevant only if the \module{Consensus} module's implementation relies on external proposal validation
    and has invoked \invoke{ValidateProposal} (see \cref{sec:module-proposal-validator}).
    If so, the \invoke{ProposalValidated} invocation contains the result of the validation
    and triggers whatever further processing of the consensus protocol that has been waiting for it.

    \item The other functions of the \module{Consensus} module are specific to its implementation.
    In our case, they handle various protocol messages
    (\invoke{Preprepare}, \invoke{Prepare}, \invoke{Commit}, \invoke{ViewChange}, etc.) and PBFT-specific timeouts.

\end{itemize}

\subsubsection{\module{Dissemination}}
\label{sec:module-dissemination}

\includegraphics[width=\textwidth]{figures/modules/module-dissemination.pdf}

Similarly to the \module{Ordering} module managing instances of \module{Consensus},
the \module{Dissemination} module manages instances of \module{Disseminator} submodules.
The main difference is that Trantor only uses a single \module{Disseminator} per epoch.

The \module{Dissemination} module has the following interface:

\begin{itemize}

    \item \invoke{NewModule(moduleConfig, retentionIndex)}\\
    When invoked by the \module{Orchestrator}, the \module{Dissemination} module instantiates a new \module{Disseminator} submodule.
    The created submodule is associated with the given retention index and parametrized according to \invoke{moduleConfig}.

    \item \invoke{GarbageCollect(retentionIndex)}\\
    When invoked by the \module{Orchestrator},
    the \module{Dissemination} module deletes all its submodules that have been created with a lower retention index than the given one.

\end{itemize}

\subsubsection{\module{Disseminator}}

\includegraphics[width=\textwidth]{figures/modules/module-disseminator.pdf}

\module{Disseminator}s are dynamically created and deleted by the \module{Dissemination} module
and are responsible for disseminating transaction payloads to other replicas (see \cref{sec:dissemination})
and producing availability certificates.
Whenever a \module{Disseminator} produces a new availability certificate, it invokes \invoke{NewCert} at the corresponding \module{Consensus} module.

A \module{Disseminator} is oblivious of the epoch it is part of
and simply executes the dissemination algorithm according to the parameters it has been given.

The \module{Disseminator} has the following interface:

\begin{itemize}

    \item \invoke{RequestCert}\\
    Requests a new availability certificate.
    In response, the \module{Disseminator} invokes \invoke{NewCert} containing the produced certificate at the \module{Consensus} module.
    Note that this need not necessarily happen immediately.
    The implementation of the \module{Disseminator} may only trigger the data dissemination upon \invoke{CertRequest}.
    In our implementation, however, the \module{Disseminator} continuously disseminates transaction data
    and buffers the resulting certificates that it can immediately use as a response to a \invoke{CertRequest}

    \item \invoke{NewBlock(block)}\\
    By invoking this function, the \module{Mempool} announces that a new transaction block is ready to be disseminated.
    The \module{Mempool} invokes \invoke{NewBlock} as a response
    to a \invoke{RequestBlock} previously invoked by the \module{Disseminator}.

    \item \invoke{BlockStored}\\
    Notifies the \module{Disseminator} that a block has been persisted in the \module{Block Store}.
    This occurs when the \module{Disseminator} has previously asked (by invoking \invoke{StoreBlock})
    the \module{Block Store} to store a block during the dissemination process.
    In our implementation, this is when the \module{Disseminator} signs the stored block and sends a \invoke{Sig} message (see \cref{sec:dissemination}).

    \item \invoke{RequestBlock(cert)}\\
    When invoked by the \module{BlockFetcher}, \module{Disseminator} looks up all the transactions (including their payloads) referenced by \invoke{cert}.
    If the referenced transactions are not present locally (in the \module{Block Store}),
    the \module{Disseminator} requests the missing transactions from replicas that have signed \invoke{cert}.
    The \module{Disseminator} then and invokes \invoke{ProvideBlock} at the \module{Block Fetcher} containing those transactions.
    
    \item \invoke{BlockLookupResult(result)}\\
    Informs the \module{Disseminator} of the result of looking up a transaction block in the \module{Block Store}.

\end{itemize}

\subsubsection{\module{Block Store}}

\includegraphics[width=\textwidth]{figures/modules/module-block-store.pdf}

The \module{Block Store} is a simple module that persistently%
\footnote{Our Go implementation uses an in-memory stub for the \module{Block Store}.}
stores transaction blocks (including payloads).
It has a simple key-value-store-like interface and has the following interface:

\begin{itemize}
    \item \invoke{StoreBlock(blockID, blockData, retentionIndex)}
    The \module{Block Store} persistently stores \invoke{blockData} under \invoke{blockID}.
    The \invoke{blockID} is derived from a hash of \invoke{blockData}.
    The stored block is associated with the given \invoke{retentionIndex} for garbage collection purposes.
    Once the block is stored, the \module{Block Store} responds with a \invoke{BlockStored} invocation.

    \item \invoke{LookUpBlock(blockID)}
    The \module{Block Store} looks up the given \invoke{blockID} in its local database and invokes \invoke{BlockLookupResult} (back at the \module{Disseminator})
    with either the corresponding block data or a flag that the block was not found.
    
    \item \invoke{GarbageCollect(retentionIndex)}
    When invoked by the \module{Orchestrator}, the \module{Block Store} module deletes all blocks associated with a retention index smaller than \invoke{retentionIndex}.
    
\end{itemize}

\subsubsection{\module{Mempool}}
\label{sec:module-mempool}

\includegraphics[width=\textwidth]{figures/modules/module-mempool.pdf}

While, as mentioned in \cref{sec:mempool}, the design of Trantor treats the mempool as an external dependency,
our implementation includes a \module{Mempool} module with a \invoke{NewTransactions} function and stores the contained transactions in memory.
Upon \invoke{RequestBlock}, the \module{Mempool} responds with a \invoke{NewBlock} containing the stored transactions.
The \module{Mempool}'s operation is influenced by several parameters%
\footnote{Other modules, not just the \module{Mempool}, also can be parametrized in various ways, but we omit the description of these parameters for simplicity.
We make an exception for the \emph{Mempool} and do describe its parameters, since they are relevant for our performance evaluation presented in \cref{sec:evaluation}.}:

\begin{itemize}
    
    \item \param{MaxTransactionsInBlock}: A \invoke{NewBlock} invocation never contains more than \param{MaxTransactionsInBlock} transactions.
    If the number of stored transactions exceeds this value, the next created block will contain the \param{MaxTransactionsInBlock} least recently added transactions.
    
    \item \param{MaxPayloadInBlock}: Analogously to \param{MaxTransactionsInBlock},
    \param{MaxPayloadInBlock} defines the maximal total cumulative payload size of all transactions in a block.
    
    \item \param{BlockTimeout}: To enable effective batching of transactions, the mempool does not respond to a \invoke{RequestNewBlock} immediately
    if there are not enough pending transactions for reaching at least one of the two above limits.
    \param{BlockTimeout} defines the maximal time the mempool waits for a block to fill.
    After \param{BlockTimeout} elapses, the mempool invokes \invoke{NewBlock} at the corresponding \module{Disseminator}, even if it is not full (or is even completely empty).
\end{itemize}

When the \module{Mempool} creates a transaction block, the block still needs to be proposed, ordered, and executed.
If the transactions end up not being executed for some reason (e.g., the ordering fails due to network asynchrony),
Trantor needs to propose a block with those transactions again.
Thus, transactions can be removed from the \module{Mempool} only after they have been executed
and the corresponding state of Trantor has been captured in a checkpoint.

At the same time, transactions that \emph{are going to be} executed (and checkpointed) should not be included in new blocks created by the \module{Mempool}.
However, the \module{Mempool} cannot know which of the already created transactions are going to make it into a checkpoint and which are not.
A simple solution would be to simply delay creating a new block until the previous one has either succeeded or failed to be executed.
This is very restrictive and may be detrimental to performance.
Instead, we make the \module{Mempool} aware of Trantor epochs and allow it to include each transaction only once per epoch.
We leverage the fact that epoch transitions are a form of synchronization points with no ``in-flight'' blocks.
At the end of an epoch, all executed transactions are checkpointed and can be safely deleted from the \module{Mempool}.
Transactions that have not been executed need to be proposed again in the next epoch.

The \module{Mempool} also keeps track of client watermarks (see \cref{sec:tx-deduplication})
and ignores all transactions that are outside of their respective clients' watermark windows.
A client that has never submitted a transaction is defined to have a watermark window starting at \textit{TxNo} 0
(no explicit ``registration'' of clients is necessary).
Trantor updates the \module{Mempool}'s watermark information at the beginning of every epoch.

The \module{Mempool} has the following interface:

\begin{itemize}

    \item \invoke{NewTransactions(transactions)}\\
    The \module{Mempool} stores all the provided \invoke{transactions} that are within their respective client's watermark windows.

    \item \invoke{RequestBlock}
    The \module{Mempool}, according to its parametrization, eventually invokes \invoke{NewBlock} at the corresponding \module{Disseminator}.

    \item \invoke{BlockTimeout}
    When waiting for a block to fill, the \module{Mempool} sets up a timer
    (by invoking \invoke{Delay} at the \module{Timer} module),
    making the \module{Timer} invoke \invoke{BlockTimeout} after \param{BlockTimeout}.
    In reaction, the \module{Mempool} invokes \invoke{NewBlock} at the \module{Disseminator} with the corresponding block, unless it has already done so.

    \item \invoke{NewEpoch(executedTXs)}
    The \module{Block Fetcher} invokes this function at the start of every epoch.
    The \module{Mempool}
    \begin{itemize}
        \item updates its view of the client watermark windows according to \invoke{executedTXs},
        \item deletes all \invoke{executedTXs}, and
        \item resets the internal trackers of already emitted transactions.
    \end{itemize}

\end{itemize}

\subsubsection{\module{Application}}

\includegraphics[width=\textwidth]{figures/modules/module-application.pdf}

The \module{Application} module simply wraps the user-defined application (see \cref{sec:execution}) in a module.
At every invocation of its interface, the \module{Application} module simply calls the corresponding function of the execution component
and, where appropriate, invokes the appropriate function of the appropriate module in response.

The \module{Application} module has the following interface:

\begin{itemize}

    \item \invoke{OrderedBlock(block)}
    The \module{Application} module calls the \code{ApplyTXs(transactions)} function of the application.

    \item \invoke{SnapshotRequest}
    The \module{Application} module calls the \code{Snapshot()} function of the application
    and invokes \invoke{Snapshot} containing its return value at the \module{Checkpoint} module
    (which is responsible for creating the checkpoint the snapshot is part of.).

    \item \invoke{RestoreState(checkpoint)}
    The \module{Application} module calls the \code{RestoreState(checkpoint)} function of the application.

    \item \invoke{NewEpoch}
    The \module{Application} module calls the \code{NewEpoch} function on the application
    and invokes \invoke{NewConfig} (containing the returned configuration) at the \module{Orchestrator}.

    \item \invoke{Checkpoint(checkpoint)}
    The \module{Application} module calls the \code{Checkpoint(checkpoint)} function on the application.

\end{itemize}

\subsubsection{\module{Block Fetcher}}

\includegraphics[width=\textwidth]{figures/modules/module-block-fetcher.pdf}

In general, it is mostly the \module{Orchestrator} that assembles a sequence of invocations of the \module{Application}.
This includes invocations for requesting state snapshots, restoring the state, and notifications about new checkpoints and epochs.
It is only the \invoke{OrderedBlock} invocations that are missing in this sequence -- the \module{Orchestrator} uses \invoke{OrderedCert}s in their place.
Moreover, instead the \module{Orchestrator} performs all these invocations on the \module{Block Fetcher} instad of on the \module{Application}.

The \module{Block Fetcher}'s main task is to replace the \invoke{OrderedCert} invocations in this sequence by the corresponding \invoke{OrderedBlock} invocations
and relay these invocations (in the same order) to the \module{Application}.
When invoking \invoke{OrderedBlock}, the \module{Block Fetcher} also removes duplicate transactions as described in \cref{sec:tx-deduplication}.

Replacing an \invoke{OrderedCert} by an \invoke{OrderedBlock} involves communication with the corresponding \module{Disseminator}
which, in turn, might even need to communicate with other replicas over the network.
In the meantime, the \module{Block Fetcher} might receive more invocations from the \module{Orchestrator}.
The \module{Block Fetcher} can buffer such invocations and delay their processing until it has finished fetching the block.
This is required if the content of the block can change the \module{Block Fetcher}'s state (e.g., the information about delivered transactions)
on which the processing of subsequent invocations depends.

The \module{Block Fetcher} keeps track of the current epoch by reacting to the \invoke{NewEpoch} invocations from the \module{Orchestrator}.
It uses the epoch information to always request the transaction blocks from the correct \module{Disseminator} instance.

The \module{Block Fetcher} has the following interface:

\begin{itemize}

    \item \invoke{OrderedCert(cert)}\\
    The \module{Block Fetcher} requests the corresponding transaction block from the current epoch's \module{Disseminator} instance
    by invoking its \invoke{RequestBlock} function.

    \item \invoke{ProvideBlock(transactions)}\\
    The \module{Disseminator} invokes this function in response to the \module{Block Fetcher}'s \invoke{RequestBlock} invocation at the \module{Disseminator}.
    In reaction, the \module{Block Fetcher} assembles a new block from only those \invoke{transactions}
    that have not yet been delivered to the \module{Application} and invokes its \invoke{OrderedBlock} function.
    The \module{Block Fetcher} then either directly invokes \invoke{OrderedBlock} at the \module{Application}
    or, in case a previous invocation is still pending to be relayed, enqueues it for later.

    \item \invoke{SnapshotRequest}
    The \module{Orchestrator} invokes this function during the creation of a checkpoint.
    The \module{Block Fetcher} relays it to the \module{Application}.
    Moreover, since the \module{Block Fetcher} also contains state than must be checkpointed (the client watermark information),
    it serializes the relevant part of its state and invokes \invoke{ClientProgress} directly at the corresponding \module{Checkpoint} module.

    \item \invoke{RestoreState(checkpoint)}
    The \module{Block Fetcher} invokes the \module{Application}'s \invoke{RestoreState(checkpoint)} and re-initializes the relevant parts of its own state
    (epoch number and client watermark information).

    \item \invoke{NewEpoch}
    The \module{Block Fetcher} updates its local view of the epoch number and relays the invocation to the \module{Application}.
    It also invokes \invoke{NewEpoch} at the \module{Mempool} with information about the transactions executed so far (see \cref{sec:module-mempool}).
    
    \item \invoke{Checkpoint}
    The \module{Block Fetcher} simply relays this invocation to the \module{Application} (in the appropriate order relative to the other relayed invocations).

\end{itemize}

\subsubsection{\module{Checkpointing}}

\includegraphics[width=\textwidth]{figures/modules/module-checkpointing.pdf}

Analogously to the \module{Dissemination} module (\cref{sec:module-dissemination}),
the \module{Checkpointing} module manages instances of the \module{Checkpoint} submodule,
each of which is responsible for creating a single checkpoint.
At the start of each epoch, the \module{Orchestrator} creates a new \module{Checkpoint} instance that assembles the starting checkpoint of that epoch.

The \module{Checkpointing} module has the following interface:

\begin{itemize}

    \item \invoke{NewModule(moduleConfig, retentionIndex)}\\
    The \module{Checkpointing} module instantiates a new \module{Checkpoint} submodule.
    The created submodule is associated with the given retention index and parametrized according to \invoke{moduleConfig}.

    \item \invoke{GarbageCollect(retentionIndex)}\\
    The \module{Checkpointing} module deletes all its submodules that have been created with a lower retention index than the given one.

\end{itemize}

\subsubsection{\module{Checkpoint}}

\includegraphics[width=\textwidth]{figures/modules/module-checkpoint.pdf}

The \module{Checkpoint} module represents a single instance of the checkpointing subprotocol described in \cref{sec:creating-checkpoints}.
Trantor creates one such instance at the start of each epoch.
Every instance creates a checkpoint corresponding to the state of the system after finishing the previous epoch
and before delivering any transactions in the current one.

Before starting the checkpointing protocol, the \module{Checkpoint} module waits for receiving the state to be checkpointed.
Part of this state (relevant state of Trantor itself and future membership configurations)
is provided directly by the \module{Orchestrator} at initialization.
The rest -- the serialized application state and the most up-to-date client watermark information --
arrives later through, respectively, a \invoke{Snapshot} and a \invoke{ClientProgress} invocation.
After having received both, the protocol starts, eventually outputting a certified checkpoint.

Different replicas might be ready to start executing the checkpointing protocol at different times.
Replicas receiving protocol messages before having themselves started the protocol buffer (a limited amount of) the early messages
and process them right after starting the protocol execution.

The \module{Checkpoint} module has the following interface:

\begin{itemize}
    \item \invoke{Snapshot(appSnapshot)}\\
    Delivers the snapshot of the application state to be checkpointed.
    If the \invoke{ClientProgress} has already been received, the module starts the checkpointing protocol.
    
    \item \invoke{ClientProgress(watermarkData)}\\
    Delivers the most up-to-date client watermark infromation.
    If the \invoke{Snapshot} has already been received, the module starts the checkpointing protocol.

    \item The other functions of the \module{Checkpoint} module are specific to its implementation.
    In our case, they handle received protocol messages.
\end{itemize}

\subsubsection{\module{Checkpoint Validator}}

\includegraphics[width=\textwidth]{figures/modules/module-checkpoint-validator.pdf}

The \module{Checkpoint Validator} validates checkpoints received over the network.
In particular, the \module{Orchestrator} requests a checkpoint validation each time when "catching up",
i.e., when it receives a checkpiont over the network.

We expect that the \module{Checkpoint} module and the \module{Checkpoint Validator} modules are developed in a coordinated way,
such that checkpoints produced by the former can be validated by the latter.

The \module{Checkpoint Validator} module has the following interface:

\begin{itemize}

    \item \invoke{ValidateCheckpoint(checkpoint)}\\
    Validates the given checkpoints and invokes \invoke{CheckpointValidated} with the validation result at the calling module.
    
\end{itemize}

\subsubsection{\module{Networking}, \module{Timer}, \module{Hashing}, \module{Crypto}}

These modules provide low-level functionality supporting many of the modules described above.
The interaction with these modules has been omitted as a technical detail.
For an intuition about their workings, however, we list their interface functions:
\begin{itemize}
    
    \item \module{Networking}
    \begin{itemize}
        \item \invoke{SendMessage(message, destinations)}
    \end{itemize}
    
    \item \module{Timer}
    \begin{itemize}
        \item \invoke{Delay(invocation, delay)}
        \item \invoke{Repeat(invocation, period, retentionIndex)}
        \item \invoke{GarbageCollect(retentionIndex)}
    \end{itemize}

    \item \module{Hashing}
    \begin{itemize}
        \item \invoke{HashRequest(data)}
    \end{itemize}

    \item \module{Crypto}
    \begin{itemize}
        \item \invoke{Sign(data)}
        \item \invoke{VerifySig(data, signer)}
    \end{itemize}
    
\end{itemize}

% \subsection{TODO}

% \begin{itemize}
%     \item Application vs Execution
%     \item Say that epoch-based operation and submodule instantiation makes it easy for the consensus instances to not care about reconfiguration.
%     \item Dynamic module creation, message buffering
%     \item Multi-threaded sub-module execution
% \end{itemize}

\section{Performance Evaluation}
\label{sec:evaluation}

This section presents a preliminary performance evaluation of Trantor, measuring the latency and throughput of ordering transactions.
The purpose is not to improve upon the state of the art in any particular metric,
but rather show that Trantor can provide more than sufficient throughput (tens of thousands oft transactions per second)
and acceptable latency (sub-second in a global deployment) for many real-world applications.
Most importantly, we show that the performance price for modularity and clean abstractions is worth paying.

For now, we only evaluate the "good-case" performance, that is, we do not artificially introduce faults or simulate Byzantine behavior of some replicas.
Mechanisms for dealing with such situations are in place, however (PBFT view change),
and we plan on evaluating Trantor's performance under adverse conditions in the future.

\subsection{Experimental Setup}

We measure Trantor's performance at a moderate scale of 32 and 64 replicas.
For each replica, we use an Amazon EC2 virtual machine of type \texttt{t4g.xlarge}
with 4 virtual Arm-based AWS Graviton2 CPUs and 16GB of memory (even though most of the memory is not used).
The replicas are either all connected by a local network (LAN) or dispersed over the globe as far apart as possible, communicating over the internet (WAN):
\begin{compactitem}
    \item \textbf{LAN:} Single Amazon datacenter in Ireland.
    \item \textbf{WAN:} 8 Amazon datacenters:
    N. Virginia (\texttt{us-east-1}),
    N. California (\texttt{us-west-1}),
    Seoul (\texttt{ap-northeast-2}),
    Sydney (\texttt{ap-southeast-2}),
    Central Canada (\texttt{ca-central-1}),
    Frankfurt (\texttt{eu-central-1}),
    Ireland (\texttt{eu-west-1}),
    and Sao Paulo (\texttt{sa-east-1}).
\end{compactitem}
For the WAN deployments, replicas are uniformly distributed among the 8 different locations,
i.e., 4 replicas and 8 replicas in each location respectively for deployments of 32 and 64 replicas.

\subsection{Load Generation Generation and Data Collection}

To generate the load, we implement simple clients that generate transactions and insert them into the replicas' mempools.
To measure purely the throughput and latency of transaction ordering, we co-locate each client with a replica on the same machine.
In fact, each client is merely a thread within the replica's process.%
\footnote{Trantor's design is very well-suited for implementing such clients --
one merely needs to add a module continuously invoking \invoke{NewTransaction} at the \module{Mempool}.}

The clients operate in a “closed loop”.
Each client submits a transaction with 512 bytes of randomly generated payload, waits until the transaction is delivered, and then immediately submits the next one.
We consider a client's transaction \textit{tx} delivered when the replica the client is located on invokes \code{ApplyTX(transactions)} with $tx \in \code{transactions}$.
In our experiments, we gradually increase the number of such clients until the system becomes saturated and the throughput stops increasing.
Most of the time, it takes tens of thousands of clients to saturate the system.

Our mempool implementation is parametrized with \param{MaxTransactionsInBlock} = 2048 (chosen empirically after exploring a few others),
\param{MaxPayloadInBlock} = 16MB (a very large value that effectively disables this limit), and a varying \param{BlockTimeout}.
The duration of each run (corresponding to one data point in the plots) is 2 minutes.

\subsection{Latency Metrics}

This preliminary evaluation focuses on the expected "good-case" scenario that users might expect during normal operation of the system.
We conduct our experiments in a simple way, gathering the data during the whole 2 minutes of each experiment's duration.
As is common in experiments where not all replicas can start up at exactly the same time,
the data from the first and last few seconds are not representative of the steady state.
Nevertheless, we include this data for simplicity.

Instead, we report on the median transaction latency as opposed to the common average or tail latency, as the median is not affected by the above.
We thus believe it is the most representative of the normal-case steady-state operation of Trantor.
Nevertheless, the difference between our measured median and average latencies small in most of the cases (within 50\%).
The tail (95th percentile) latency is rarely more than twice the average.

Since each client is co-located with a replica and only submits its transactions to that single replica,
we really only measure Trantor's ordering overhead, not the overall end-to-end latency of a real-world application built on Trantor.
Also, the delivery of a transaction by a replica that proposed it (when we measure latency)
does not guarantee that the same transaction has already been delivered by other replicas as well.

In some real-world applications, where the client is remote and does not trust any single replica,
the end-to-end latency would increase by
1) the time until sufficiently many replicas deliver the transaction and
2) the communication delay between the client and the slowest of these replicas.

\subsection{Results}

\begin{figure}
    \centering
    \includegraphics[width=0.79\textwidth]{figures/results-wan-final.png}
    \caption{\centering Median latency (in seconds) vs. throughput (in tx/s)\newline
    in a WAN deployment of 32 and 64 replicas.}
    \label{fig:eval-wan}
\end{figure}

\Cref{fig:eval-wan} shows Trantor's performance when deployed across the globe.
For 32 replicas, Trantor achieves a sub-second latency for throughputs of up to 10'000 tx/s,
and under a load of up to 8'000 concurrent clients (throughput of 4000' tx/s), Trantor's median ordering latency is as low as 0.5 seconds.
Trantor keeps a latency under 1.3 seconds until it reaches its peak throughput of above 30'000 tx/s with around 50'000 concurrent clients.
At this point, the system saturates and an increased number of clients only results in an increased latency.
When increasing the number of clients even further, even the throughput starts decreasing due to thrashing effects.
We chose the \param{BlockTimeout} of 0.4 seconds empirically,
as it provided the best results after a brief (and non-exhaustive) search of the parameter space.

A WAN deployment of 64 replicas yields slightly higher latency and, interestingly, a comparable peak throughput.
This suggests that the network bandwidth is not the bottleneck, at least for the 32-replica deployment.
Indeed, The average bandwidth used per replica at saturation is 16 MiB/s and 21 MiB/s respectively for the small and large scale deployment.

Another interesting phenomenon occurs at throughputs between 10'000 and ca. 17'000 tx/s, where both system sizes exhibit the same latencies.
We attribute this to the different setting of the \param{BlockTimeout} parameter for the two different system sizes,
as we increase the \param{BlockTimeout} at larger scale to 0.8 s to compensate for the higher message processing overhead.%
\footnote{Due to the rather large block size limit of 2048 transactions,
most of the blocks in our experiments are not completely full and are triggered by the \invoke{BlockTimeout}, especially before the system is saturated.}

\begin{figure}
    \centering
    \includegraphics[width=0.79\textwidth]{figures/results-lan-final.png}
    \caption{\centering Median latency (in seconds) vs. throughput (in tx/s)
    in a LAN deployment of 32 and 64 replicas}
    \label{fig:eval-lan}
\end{figure}

% \paragraph{\param{SegmentLength} parameter.} Being heavily inspired by ISS \cite{iss},
% Trantor's ordering protocol subdivides each epoch into so-called segments.
% In a nutshell, a segment is a subset of sequence numbers within one epoch for which the same replica is disseminating and proposing a transaction block
% (i.e., in our case, serves as the PBFT primary).
% The union of all segments makes up the whole epoch.
% The \param{SegmentLength} is the number of blocks each replica needs to propose (and have the system agree upon) in each epoch.
% Thus, for example, for a \param{SegmentLength} of 2 and 32 replicas, the epoch length is 64
% and each replica proposes (concurrently with other replicas) 2 blocks before the next epoch can start.

% Since Trantor creates a checkpoint at each epoch transition,
% shorter segments generally mean more checkpointing overhead and an increased good-case latency,
% as the system must wait more often for a checkpoint before starting a new epoch.
% However, longer epochs increase the risk of more replicas falling behind during one epoch
% and introducing instability in the system that might lead to thrashing, especially in large deployments.
% If a single replica is slow at, say, disseminating transaction blocks,
% all other replicas need to wait for it at the end of the epoch,
% which has a significant impact on the latency of transactions waiting in the mempool.
% For all experiments except for one, we use a \param{SegmentLength} of 2.

\Cref{fig:eval-lan} shows Trantor's performance when deployed in a single datacenter with a fast network.
For 32 replicas, until the system starts becoming saturated at 60'000 tx/s, Trantor's latency is mostly under 0.25 seconds.
For a deployment of 64 replicas, throughput drops to 40'000 tx/s and latency increases to just below 1 second.
This is because each replica needs to wait for a larger quorum in order to make progress in both the dissemination and the ordering protocol.
Also, the overhead of ordering every transaction is higher, as each replica needs to send more outgoing messages and process more incoming ones.
This has a strong impact both on the network bandwidth (mostly for the dissemination component) and CPU load (mostly the ordering component).
We partially compensate for the higher CPU load by increasing the \param{BlockTimeout} to 0.8 seconds,
but it does not seem to be sufficient on a LAN.
We attribute this to a bandwidth bottleneck since, at saturation, both the small and large scale deployments
rarely exceed an average network transmission rate of 35 MiB/s per replica.

\subsection{Discussion}

In the following, we discuss some additional noteworthy aspects of Trantor's performance evaluation.

\paragraph{LAN Latency.}
When deployed in a single datacenter, Trantor's latency is higher than one might expect from a system deployed on such a fast network.
In fact, for 64 replicas, the LAN latency is almost as high as in the WAN.
This has two main reasons.

First, the algorithms used are not latency-optimal.
The very rudimentary implementation of the dissemination component adds an extra round-trip (quorum gathering) to the latency of PBFT.
This can, however, be alleviated by using better protocols for dissemination and ordering, e.g., as is done in Bullshark \cite{bullshark}.

Second, Trantor's current implementation focuses on feature-completeness, not on processing performance.
Therefore, it is very likely that local message processing also has a non-negligible contribution to overall transaction latency.

\paragraph{Transaction Deduplication.}
In a scenario like the above, a client might submit the same transaction to multiple replicas' mempools.
As described in \cref{sec:tx-deduplication}, the current implementation of Trnator would treat them as separate transactions,
lowering the effective throughput.
In our experiments, such duplication does not occur, since we also do not expect it to occur during normal operation in a real-world deployment.
Nevertheless, a slightly modified version of Trnator's dissemination and ordering components
can easily implement a deduplication mechanism \cite{mirbft,iss} to mitigate this issue.

\paragraph{Persistent Storage.}
Our implementation of the \module{Block Store} is a stub that does not persistently store any data.
Instead, we use a simple in-memory hash map to store the blocks.
The overhead of accessing stable storage is thus not taken into account in our performance evaluation.

\paragraph{Access Control and Input Validation.}
As discussed in \cref{sec:access-control-input-validation}, Trantor does not, by default, perform access control or input validation
(although it can be configured to do so).
Thus, our evaluation does not take into account it's potential overhead.

\paragraph{Unoptimized Implementation.}
Trantor's current implementation focuses on modularity and feature-completeness rather than on immediate performance optimization.
We prefer having smaller, well-encapsulated modules with simple yet meaningful interfaces
that significantly simplify reasoning about both their interaction and implementation.
For example, fetching missing transactions has a very simple interface (\invoke{RequestBlock}/\invoke{ProvideBlock}),
but its implementation is the most naive possible: requesting all transactions referenced by the certificate from all replicas that signed it,
introducing manyfold redundancy.

Our Go implementation also uses (for historical reasons) an extremely inefficient representation of messages based on Protocol Buffers,
where simple deserialization of a received message involves more than 10 (in some cases potentially more than 20) memory allocations.

Trantor achieves the presented performance despite these and many more inefficiencies.
More importantly, Trantor makes it easy to optimize each component separately in the future.
Even this document mentions several improvements that can be implemented in a future version of Trantor.
Trantor's architecture makes it easy to do so with only minimal modifications (if any at all) to the present system.
If deemed necessary, even optimizations that intrinsically require breaking abstraction boundaries
can be implemented in Trantor by replacing multiple involved components by a single, more integrated one.

\section{Conclusions}
\label{sec:conclusions}

Trantor is a state machine replication (SMR) system that allows an arbitrary application modeled as a deterministic state machine
to be deployed across multiple replicas in a fault-tolerant way.
Trantor’s design focuses on modularity.
We decompose the problem of SMR at the conceptual level into smaller sub-problems, each of which can be studied, reasoned about, and implemented separately.
We design the solution to each of these sub-problems as a separate component of Trantor.
In the future, it should be easy to replace or rearrange these components to adapt the system to varying requirements and new state of the art.

A preliminary good-case performance evaluation of Trnator shows very promising results
with a throughput in the tens of thousands tx/s and sub-second latency,
even when deployed over a WAN across the globe.
All this despite its simple implementation that still leaves a very large potential for performance optimization.

Trantor does not claim novelty in terms of protocols used or performance achieved.
In fact, the implementation of some of Trantor’s components is naive and sub-optimal compared to what is achievable in theory (and in practice).
It is not (yet) the point of Trantor to employ the most state-of-the-art component implementations, but to enable anyone to do so easily.
The ultimate goal is to be able to cherry-pick existing solutions to the multitude of sub-problems in SMR and flexibly combine them within one SMR system.

\newpage

\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}
